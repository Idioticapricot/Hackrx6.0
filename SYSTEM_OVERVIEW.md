# LegalEase AI - System Overview

## ğŸ—ï¸ Architecture

**Frontend**: Static HTML/CSS/JS with 3 tabs (Simplified View, Risk Analysis, Q&A)
**Backend**: FastAPI with async processing
**AI**: Google Gemini 1.5 Flash + MiniLM embeddings
**Strategy**: Hybrid approach - Full context for analysis, RAG for Q&A

## ğŸ¤– AI Models Used

### Primary LLM
- **Google Gemini 1.5 Flash** - Document analysis, simplification, risk detection, Q&A
- **Context Window**: ~1M tokens (allows full document processing)
- **API**: Google Generative AI REST API

### Embeddings & Retrieval
- **all-MiniLM-L6-v2** (`sentence-transformers/all-MiniLM-L6-v2`) - 384-dim lightweight embeddings
- **Jina Reranker v1 Tiny** (`jinaai/jina-reranker-v1-tiny-en`) - Compact reranking model
- **FAISS** - Vector similarity search
- **Device**: CPU optimized for MacBook (batch_size: 16)

## ğŸ“Š Processing Flow

### Document Analysis (Simplified View + Risk Analysis)
```
Document URL â†’ Download â†’ Extract Text â†’ Send FULL CONTEXT to Gemini â†’ Analysis
```
- Uses entire document content (up to ~8000 chars sent to API)
- Single API call for comprehensive analysis
- Better for understanding document structure and relationships

### Q&A Processing
```
Document URL â†’ Chunk â†’ Embed â†’ FAISS Index â†’ Query â†’ Retrieve â†’ Rerank â†’ Answer
```
- Traditional RAG pipeline for targeted question answering
- Retrieves top-10 relevant chunks per question
- More efficient for specific questions

## âš™ï¸ Key Configuration

### API Settings
- **Model**: `gemini-1.5-flash`
- **Max Output Tokens**: 2048 (bottleneck for long simplifications)
- **Temperature**: 0.3
- **Safety Filters**: Disabled for legal content

### Processing
- **Chunk Size**: 800 tokens, 200 overlap
- **Small Doc Threshold**: 5000 tokens (bypass RAG)
- **Batch Size**: 16 (CPU optimized)

### Rate Limits
- **LLM Calls**: 20 concurrent
- **Image Calls**: 1 concurrent

## ğŸ”„ Request Flow

1. **Document Upload** â†’ URL validation
2. **Document Processing** â†’ Download, extract, chunk
3. **Strategy Selection**:
   - **Legal Analysis**: Full context â†’ Gemini
   - **Q&A**: RAG pipeline â†’ Context retrieval â†’ Gemini
4. **Response Generation** â†’ Format and return

## ğŸš¨ Known Issues

### Token Limit Bottleneck
- **Problem**: 2048 max tokens insufficient for document simplification
- **Impact**: Truncated responses appear as "content blocked"
- **Solution**: Increase to 8192+ tokens for simplification

### Safety Filters
- **Disabled**: All Gemini safety categories set to `BLOCK_NONE`
- **Reason**: Legal content often triggers false positives

## ğŸ¯ Optimization Points

1. **Token Limits**: Different limits per endpoint type
2. **Chunking**: Optimize chunk size for legal documents
3. **Caching**: Persistent embeddings and response caching
4. **Error Handling**: Better distinction between truncation vs blocking

## ğŸ“ File Structure

```
src/
â”œâ”€â”€ ai/                 # Gemini client, prompts
â”œâ”€â”€ api/                # FastAPI endpoints
â”œâ”€â”€ core/               # Configuration (heart of system)
â”œâ”€â”€ document_processing/ # Loaders, chunking
â”œâ”€â”€ models/             # Pydantic schemas
â”œâ”€â”€ utils/              # Legal analyzer, helpers
â””â”€â”€ static/             # Frontend files
```

## ğŸ”§ Quick Fixes Needed

1. **Increase maxOutputTokens** to 8192 for simplification
2. **Add response length validation** before sending to frontend
3. **Implement chunked simplification** for very large documents